{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d920d7f3-f314-451d-9e44-29a0bb3449f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyreadstat in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: narwhals>=2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyreadstat) (2.12.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyreadstat) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyreadstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5f34103-f939-40ed-bf79-35b12fee8bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found XPT files:\n",
      " - ALB_CR_L.xpt\n",
      " - BAX_L.xpt\n",
      " - BMX_L.xpt\n",
      " - BPQ_L.xpt\n",
      " - BPXO_L.xpt\n",
      " - DEMO_L.xpt\n",
      " - DIQ_L.xpt\n",
      " - GHB_L.xpt\n",
      " - GLU_L.xpt\n",
      " - HDL_L.xpt\n",
      " - INS_L.xpt\n",
      " - PAQ_L.xpt\n",
      " - SMQ_L.xpt\n",
      " - TRIGLY_L.xpt\n",
      "\n",
      "=====================================\n",
      "\n",
      "[OK] Loaded ALB_CR_L.xpt → 8493 rows, 8 columns\n",
      "[OK] Loaded BAX_L.xpt → 4771 rows, 45 columns\n",
      "[OK] Loaded BMX_L.xpt → 8860 rows, 22 columns\n",
      "[OK] Loaded BPQ_L.xpt → 8501 rows, 6 columns\n",
      "[OK] Loaded BPXO_L.xpt → 7801 rows, 12 columns\n",
      "[OK] Loaded DEMO_L.xpt → 11933 rows, 27 columns\n",
      "[OK] Loaded DIQ_L.xpt → 11744 rows, 9 columns\n",
      "[OK] Loaded GHB_L.xpt → 7199 rows, 3 columns\n",
      "[OK] Loaded GLU_L.xpt → 3996 rows, 4 columns\n",
      "[OK] Loaded HDL_L.xpt → 8068 rows, 4 columns\n",
      "[OK] Loaded INS_L.xpt → 3996 rows, 5 columns\n",
      "[OK] Loaded PAQ_L.xpt → 8153 rows, 8 columns\n",
      "[OK] Loaded SMQ_L.xpt → 9015 rows, 9 columns\n",
      "[OK] Loaded TRIGLY_L.xpt → 3996 rows, 10 columns\n",
      "\n",
      "Loaded datasets: ['ALB_CR_L', 'BAX_L', 'BMX_L', 'BPQ_L', 'BPXO_L', 'DEMO_L', 'DIQ_L', 'GHB_L', 'GLU_L', 'HDL_L', 'INS_L', 'PAQ_L', 'SMQ_L', 'TRIGLY_L']\n",
      "\n",
      "Using 'DEMO_L' as base dataset.\n",
      "\n",
      "Merging ALB_CR_L → adding 7 new columns\n",
      "Merging BAX_L → adding 44 new columns\n",
      "Merging BMX_L → adding 21 new columns\n",
      "Merging BPQ_L → adding 5 new columns\n",
      "Merging BPXO_L → adding 11 new columns\n",
      "Merging DIQ_L → adding 8 new columns\n",
      "Merging GHB_L → adding 2 new columns\n",
      "Merging GLU_L → adding 3 new columns\n",
      "Merging HDL_L → adding 3 new columns\n",
      "Merging INS_L → adding 4 new columns\n",
      "Merging PAQ_L → adding 7 new columns\n",
      "Merging SMQ_L → adding 8 new columns\n",
      "Merging TRIGLY_L → adding 9 new columns\n",
      "\n",
      "=====================================\n",
      "Final merged dataset saved as NHANES_MERGED.csv\n",
      "Rows: 11933  Columns: 159\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyreadstat\n",
    "import os\n",
    "\n",
    "# === 1. Find all XPT files in the same folder ===\n",
    "DATA_DIR = os.getcwd()\n",
    "files = [f for f in os.listdir(DATA_DIR) if f.lower().endswith(\".xpt\")]\n",
    "\n",
    "print(\"Found XPT files:\")\n",
    "for f in files:\n",
    "    print(\" -\", f)\n",
    "\n",
    "print(\"\\n=====================================\\n\")\n",
    "\n",
    "# === 2. Safe loader (works on all pyreadstat versions) ===\n",
    "def load_xpt(file):\n",
    "    try:\n",
    "        df, meta = pyreadstat.read_xport(\n",
    "            file,\n",
    "            encoding=\"latin1\",   # safest for NHANES\n",
    "        )\n",
    "        print(f\"[OK] Loaded {file} → {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"[FAILED] {file} → {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# === 3. Load all XPT files into a dictionary ===\n",
    "dfs = {}\n",
    "for f in files:\n",
    "    key = f.replace(\".xpt\", \"\").replace(\".XPT\", \"\")\n",
    "    df = load_xpt(f)\n",
    "    if df is not None:\n",
    "        dfs[key] = df\n",
    "\n",
    "print(\"\\nLoaded datasets:\", list(dfs.keys()))\n",
    "\n",
    "\n",
    "# === 4. Find DEMO file (base for merge) ===\n",
    "demo_key = None\n",
    "for k in dfs.keys():\n",
    "    if k.lower().startswith(\"demo\"):  \n",
    "        demo_key = k\n",
    "        break\n",
    "\n",
    "if demo_key is None:\n",
    "    raise Exception(\"No DEMO file found. NHANES cannot be merged without DEMO_xxx.XPT\")\n",
    "\n",
    "print(f\"\\nUsing '{demo_key}' as base dataset.\\n\")\n",
    "\n",
    "merged = dfs[demo_key].copy()\n",
    "\n",
    "\n",
    "# === 5. Merge all other tables by SEQN ===\n",
    "for key, df in dfs.items():\n",
    "    if key == demo_key:\n",
    "        continue\n",
    "\n",
    "    # Avoid duplicated columns except SEQN\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "    old_cols = set(merged.columns)\n",
    "    new_cols = set(df.columns) - {\"SEQN\"}\n",
    "\n",
    "    print(f\"Merging {key} → adding {len(new_cols)} new columns\")\n",
    "\n",
    "    merged = merged.merge(df, on=\"SEQN\", how=\"left\")\n",
    "\n",
    "\n",
    "# === 6. Save final merged dataset ===\n",
    "merged.to_csv(\"NHANES_MERGED.csv\", index=False)\n",
    "print(\"\\n=====================================\")\n",
    "print(\"Final merged dataset saved as NHANES_MERGED.csv\")\n",
    "print(\"Rows:\", merged.shape[0], \" Columns:\", merged.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b43f23a6-3401-4b7c-8de3-dbf9baae646b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINAL COLUMNS KEPT:\n",
      " - SEQN: Participant ID\n",
      " - RIDAGEYR: Age (years)\n",
      " - RIAGENDR: Gender (1=Male,2=Female)\n",
      " - RIDRETH1: Race/Ethnicity\n",
      " - INDFMPIR: Income-to-poverty ratio\n",
      " - LBXGH: HbA1c (%)\n",
      " - LBDGLUSI: Fasting plasma glucose (mg/dL)\n",
      " - LBDLDL: LDL cholesterol (mg/dL)\n",
      " - URXUMA: Urine albumin (mg/L)\n",
      " - BMXBMI: BMI (kg/m2)\n",
      " - BMXWAIST: Waist circumference (cm)\n",
      " - DIQ010: Doctor told you have diabetes\n",
      " - DIQ050: Taking insulin now\n",
      " - BPXOSY1: Blood pressure field: BPXOSY1\n",
      " - BPXOSY2: Blood pressure field: BPXOSY2\n",
      " - BPXOSY3: Blood pressure field: BPXOSY3\n",
      " - BPXODI1: Blood pressure field: BPXODI1\n",
      " - BPXODI2: Blood pressure field: BPXODI2\n",
      " - BPXODI3: Blood pressure field: BPXODI3\n",
      " - BPXOPLS1: Blood pressure field: BPXOPLS1\n",
      " - BPXOPLS2: Blood pressure field: BPXOPLS2\n",
      " - BPXOPLS3: Blood pressure field: BPXOPLS3\n",
      " - BPAOARM: Blood pressure field: BPAOARM\n",
      " - BPAOCSZ: Blood pressure field: BPAOCSZ\n",
      " - SBP_mean: Avg systolic BP (mmHg)\n",
      " - DBP_mean: Avg diastolic BP (mmHg)\n",
      " - PP: Pulse pressure\n",
      " - MAP: Mean arterial pressure\n",
      "\n",
      "Total columns kept: 28\n",
      "\n",
      "Saved → NHANES_DIABETES_CLEAN.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"NHANES_MERGED.csv\")\n",
    "\n",
    "# ====================================================================\n",
    "# Helper function: choose first available variable from multiple names\n",
    "# ====================================================================\n",
    "def pick(df, names):\n",
    "    for n in names:\n",
    "        if n in df.columns:\n",
    "            return n\n",
    "    return None\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# 1. Define variables to keep (with comments)\n",
    "# ====================================================================\n",
    "\n",
    "keep = {}\n",
    "\n",
    "# Demographics\n",
    "keep[\"SEQN\"] = \"Participant ID\"\n",
    "keep[pick(df, [\"RIDAGEYR\"])] = \"Age (years)\"\n",
    "keep[pick(df, [\"RIAGENDR\"])] = \"Gender (1=Male,2=Female)\"\n",
    "keep[pick(df, [\"RIDRETH1\"])] = \"Race/Ethnicity\"\n",
    "keep[pick(df, [\"INDFMPIR\"])] = \"Income-to-poverty ratio\"\n",
    "\n",
    "# Diabetes labs\n",
    "keep[pick(df, [\"LBXGH\"])] = \"HbA1c (%)\"\n",
    "keep[pick(df, [\"LBDGLUSI\", \"LBXGLU\"])] = \"Fasting plasma glucose (mg/dL)\"\n",
    "keep[pick(df, [\"LBDLDL\"])] = \"LDL cholesterol (mg/dL)\"\n",
    "keep[pick(df, [\"URXUMA\"])] = \"Urine albumin (mg/L)\"\n",
    "\n",
    "# Anthropometrics\n",
    "keep[pick(df, [\"BMXBMI\"])] = \"BMI (kg/m2)\"\n",
    "keep[pick(df, [\"BMXWAIST\"])] = \"Waist circumference (cm)\"\n",
    "\n",
    "# Diabetes questionnaire\n",
    "keep[pick(df, [\"DIQ010\"])] = \"Doctor told you have diabetes\"\n",
    "keep[pick(df, [\"DIQ050\"])] = \"Taking insulin now\"\n",
    "\n",
    "# ====================================================================\n",
    "# 2. Blood Pressure (BPXO) Integration\n",
    "# ====================================================================\n",
    "\n",
    "# Raw systolic and diastolic readings\n",
    "bp_vars = [\n",
    "    \"BPXOSY1\",\"BPXOSY2\",\"BPXOSY3\",   # Systolic\n",
    "    \"BPXODI1\",\"BPXODI2\",\"BPXODI3\",   # Diastolic\n",
    "    \"BPXOPLS1\",\"BPXOPLS2\",\"BPXOPLS3\",# Pulse rate\n",
    "    \"BPAOARM\",                       # Arm used\n",
    "    \"BPAOCSZ\"                        # Cuff size\n",
    "]\n",
    "\n",
    "for v in bp_vars:\n",
    "    if v in df.columns:\n",
    "        keep[v] = f\"Blood pressure field: {v}\"\n",
    "\n",
    "# ====================================================================\n",
    "# 3. Create cleaned BP variables\n",
    "# ====================================================================\n",
    "\n",
    "df[\"SBP_mean\"] = df[[\"BPXOSY1\",\"BPXOSY2\",\"BPXOSY3\"]].mean(axis=1)\n",
    "df[\"DBP_mean\"] = df[[\"BPXODI1\",\"BPXODI2\",\"BPXODI3\"]].mean(axis=1)\n",
    "df[\"PP\"] = df[\"SBP_mean\"] - df[\"DBP_mean\"]\n",
    "df[\"MAP\"] = df[\"DBP_mean\"] + (df[\"PP\"] / 3)\n",
    "\n",
    "# Add descriptions\n",
    "keep[\"SBP_mean\"] = \"Avg systolic BP (mmHg)\"\n",
    "keep[\"DBP_mean\"] = \"Avg diastolic BP (mmHg)\"\n",
    "keep[\"PP\"] = \"Pulse pressure\"\n",
    "keep[\"MAP\"] = \"Mean arterial pressure\"\n",
    "\n",
    "# ====================================================================\n",
    "# 4. Drop all other columns\n",
    "# ====================================================================\n",
    "\n",
    "final_cols = list(keep.keys())\n",
    "df2 = df[final_cols].copy()\n",
    "\n",
    "# ====================================================================\n",
    "# 5. Print summary of columns kept + descriptions\n",
    "# ====================================================================\n",
    "\n",
    "print(\"\\nFINAL COLUMNS KEPT:\")\n",
    "for col, desc in keep.items():\n",
    "    print(f\" - {col}: {desc}\")\n",
    "\n",
    "print(\"\\nTotal columns kept:\", len(df2.columns))\n",
    "\n",
    "# ====================================================================\n",
    "# 6. Save cleaned dataset\n",
    "# ====================================================================\n",
    "\n",
    "df2.to_csv(\"NHANES_DIABETES_CLEAN.csv\", index=False)\n",
    "print(\"\\nSaved → NHANES_DIABETES_CLEAN.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a9469ae-0139-4b0b-b58e-009cfb69c0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropna:\n",
      "Rows: 11933 Columns: 28\n",
      "\n",
      "After dropna (any null removed):\n",
      "Rows: 319 Columns: 28\n",
      "\n",
      "Saved as NHANES_BP_CLEAN_DROPNA.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned BP-integrated dataset from your previous cell\n",
    "df = pd.read_csv(\"NHANES_DIABETES_CLEAN.csv\")\n",
    "\n",
    "print(\"Before dropna:\")\n",
    "print(\"Rows:\", df.shape[0], \"Columns:\", df.shape[1])\n",
    "\n",
    "# ---- DROP ANY ROW THAT CONTAINS A NULL VALUE ----\n",
    "df_dropna = df.dropna()\n",
    "\n",
    "print(\"\\nAfter dropna (any null removed):\")\n",
    "print(\"Rows:\", df_dropna.shape[0], \"Columns:\", df_dropna.shape[1])\n",
    "\n",
    "# Save the strict-clean version\n",
    "df_dropna.to_csv(\"NHANES_BP_CLEAN_DROPNA.csv\", index=False)\n",
    "\n",
    "print(\"\\nSaved as NHANES_BP_CLEAN_DROPNA.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea39da0d-b2e2-40c1-bdf2-a472d624ad8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: (11933, 28)\n",
      "After dropping only critical missing rows: (3522, 28)\n",
      "Saved as NHANES_CLEAN_METHOD2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_3248\\2183378303.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2[col] = df2[col].fillna(df2[col].median())\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_3248\\2183378303.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2[col] = df2[col].fillna(df2[col].mode()[0])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"NHANES_DIABETES_CLEAN.csv\")\n",
    "\n",
    "# Key variables needed for prediction/label\n",
    "critical_vars = [\n",
    "    \"DIQ010\",\n",
    "    \"LBDGLUSI\",\n",
    "    \"LBXGH\",\n",
    "    \"BMXBMI\",\n",
    "    \"SBP_mean\",\n",
    "    \"DBP_mean\"\n",
    "]\n",
    "\n",
    "# Drop rows missing any critical variable\n",
    "df2 = df.dropna(subset=critical_vars)\n",
    "\n",
    "print(\"Before:\", df.shape)\n",
    "print(\"After dropping only critical missing rows:\", df2.shape)\n",
    "\n",
    "# Median/mode impute the rest\n",
    "for col in df2.columns:\n",
    "    if df2[col].dtype in [\"float64\", \"int64\"]:\n",
    "        df2[col] = df2[col].fillna(df2[col].median())\n",
    "    else:\n",
    "        df2[col] = df2[col].fillna(df2[col].mode()[0])\n",
    "\n",
    "df2.to_csv(\"NHANES_CLEAN_METHOD2.csv\", index=False)\n",
    "print(\"Saved as NHANES_CLEAN_METHOD2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e744ff5-ccd7-45d6-b36f-3808fb6ba802",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_3248\\241369530.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df3[col] = df3[col].fillna(df3[col].median())\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_3248\\241369530.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df3[col] = df3[col].fillna(df3[col].mode()[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as NHANES_CLEAN_METHOD3.csv\n",
      "Final shape: (11740, 28)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"NHANES_DIABETES_CLEAN.csv\")\n",
    "\n",
    "# Drop only missing DIQ010 (label)\n",
    "df3 = df.dropna(subset=[\"DIQ010\"])\n",
    "\n",
    "# Impute all remaining\n",
    "for col in df3.columns:\n",
    "    if df3[col].dtype in [\"float64\", \"int64\"]:\n",
    "        df3[col] = df3[col].fillna(df3[col].median())\n",
    "    else:\n",
    "        df3[col] = df3[col].fillna(df3[col].mode()[0])\n",
    "\n",
    "df3.to_csv(\"NHANES_CLEAN_METHOD3.csv\", index=False)\n",
    "print(\"Saved as NHANES_CLEAN_METHOD3.csv\")\n",
    "print(\"Final shape:\", df3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff301a7-8f3b-4be3-bd66-e9d090ea003f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
